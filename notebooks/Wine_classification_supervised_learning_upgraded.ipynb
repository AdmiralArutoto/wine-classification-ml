{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Classification with Supervised Learning\n",
    "\n",
    "This project builds and evaluates supervised learning models to classify wines into three quality-related classes based on their chemical properties.\n",
    "\n",
    "We follow a complete **ML workflow**:\n",
    "\n",
    "1. Load and explore the data  \n",
    "2. Perform exploratory data analysis (EDA)  \n",
    "3. Engineer features and handle correlations  \n",
    "4. Train and tune multiple models using cross-validation  \n",
    "5. Select a final model and evaluate it on a held-out test set  \n",
    "6. Compare with additional baseline models and discuss results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem and Dataset\n",
    "\n",
    "We use the classic **Wine** dataset, where each row represents a single wine sample with numeric features such as alcohol, malic acid, magnesium, flavanoids, and more.  \n",
    "The target variable `target` encodes the wine class (three possible classes).\n",
    "\n",
    "Our goal is to learn a model that, given the chemical composition of a wine, predicts its class as accurately as possible.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- **Task type:** Multiclass classification  \n",
    "- **Features:** 13 continuous chemical measurements  \n",
    "- **Target:** `target` âˆˆ {0, 1, 2} (wine classes)  \n",
    "- **Metric focus:** F1-macro and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loading\n",
    "\n",
    "We load separate training and test sets.  \n",
    "On a real project or in production code, you would typically:\n",
    "\n",
    "- Store the CSV files in the project repository, or  \n",
    "- Download them from a known URL.\n",
    "\n",
    "Here we assume the CSVs are available on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test and train\n",
    "df_train = pd.read_csv('drive/MyDrive/DataBasesForMachineLearning/wine_train.csv')\n",
    "df_test = pd.read_csv('drive/MyDrive/DataBasesForMachineLearning/wine_test.csv')\n",
    "\n",
    "#5 first lines of the test and train\n",
    "print(\"TRAIN SET:\")\n",
    "display(df_train.head())\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "display(df_test.head())\n",
    "df_train.info()\n",
    "df_train_original = df_train.copy()\n",
    "df_test_original = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "First, we explore the training data to understand:\n",
    "\n",
    "- Feature correlations  \n",
    "- Feature distributions and potential outliers  \n",
    "- Class balance of the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_train.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('correlation')\n",
    "plt.show()\n",
    "#We will check the correlation between different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.hist(bins=15, figsize=(15, 10), color='skyblue', edgecolor='black')\n",
    "plt.suptitle('Distribution of Features')\n",
    "plt.show()\n",
    "# We will identify outliers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the suspects of having outliers\n",
    "suspicious_features = ['malic_acid', 'magnesium', 'nonflavanoid_phenols',\n",
    "                       'od280/od315_of_diluted_wines', 'proline']\n",
    "\n",
    "# draw a plotbox for each one of them\n",
    "for feature in suspicious_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(data=df_train, x='target', y=feature)\n",
    "    plt.title(f'{feature} distribution by wine class')\n",
    "    plt.xlabel('Wine Class (target)')\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=df_train, palette='pastel')\n",
    "plt.title('Distribution of Wine Classes (target)')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n",
    "#check if there are enough samples of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering and correlation handling\n",
    "\n",
    "From the correlation heatmap, we identify highly correlated features.  \n",
    "In particular, `flavanoids` is strongly correlated with `total_phenols` and also with the target.\n",
    "\n",
    "To reduce redundancy and potential overfitting, we drop `flavanoids` from the feature set and recompute the correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_original.drop(['flavanoids'], axis=1, inplace=True)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_train_original.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Corolation')\n",
    "plt.show()\n",
    "#we saw a high corrolation between flavanoids and total_phenoids so we removed flavanoids because it has a higher correlation with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model selection and hyperparameter tuning\n",
    "\n",
    "We frame model selection as an experiment between:\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)** with standardized features  \n",
    "- **Decision Tree** with entropy criterion  \n",
    "\n",
    "We use `GridSearchCV` with 5-fold cross-validation and optimize for **F1-macro**, which balances performance across all three classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data\n",
    "X = df_train_original.drop('target', axis=1)\n",
    "y = df_train_original['target']\n",
    "\n",
    "# pipeline for each model\n",
    "pipeline_knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipeline_tree = Pipeline([\n",
    "    ('tree', DecisionTreeClassifier(random_state=42, criterion='entropy'))\n",
    "])\n",
    "\n",
    "# checking multiple hyper parameters\n",
    "param_grid_knn = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "param_grid_tree = {\n",
    "    'tree__max_depth': [3, 5, 7],\n",
    "    'tree__min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# GridSearch for the best hyper parameters\n",
    "grid_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, scoring='f1_macro')\n",
    "grid_tree = GridSearchCV(pipeline_tree, param_grid_tree, cv=5, scoring='f1_macro')\n",
    "\n",
    "grid_knn.fit(X, y)\n",
    "grid_tree.fit(X, y)\n",
    "\n",
    "# Final table and outcomes\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'K-Nearest Neighbors',\n",
    "        'Best Params': grid_knn.best_params_,\n",
    "        'F1-macro Score': round(grid_knn.best_score_, 4)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Decision Tree',\n",
    "        'Best Params': grid_tree.best_params_,\n",
    "        'F1-macro Score': round(grid_tree.best_score_, 4)\n",
    "    }\n",
    "])\n",
    "print(\"\\nBest Parameters for KNN:\")\n",
    "print(grid_knn.best_params_)\n",
    "\n",
    "print(\"\\nBest Parameters for Decision Tree:\")\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(comparison_df)\n",
    "\n",
    "best_model_row = comparison_df.loc[comparison_df['F1-macro Score'].idxmax()]\n",
    "print(f\"\\nBest Model: {best_model_row['Model']}\")\n",
    "print(f\"Best F1-macro Score: {best_model_row['F1-macro Score']}\")\n",
    "print(f\"Best Parameters: {best_model_row['Best Params']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final model training\n",
    "\n",
    "Based on the cross-validation experiment, we select the best-performing configuration.  \n",
    "Here, we train a final `KNeighborsClassifier` model using the chosen hyperparameters on the full training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x and y train for the training of the model\n",
    "X_train = df_train_original.drop('target', axis=1)\n",
    "y_train = df_train_original['target']\n",
    "\n",
    "#building the model with the hyper parameters chosen\n",
    "final_knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=3,\n",
    "        weights='uniform',\n",
    "        metric='manhattan'\n",
    "    ))\n",
    "])\n",
    "final_knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on held-out test set\n",
    "\n",
    "We apply the same preprocessing step (dropping `flavanoids`) to the test set, evaluate the final model, and report:\n",
    "\n",
    "- Classification report (per-class precision, recall, F1)  \n",
    "- Overall F1-macro  \n",
    "- Overall accuracy  \n",
    "- Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing flavanoids in the test data base:\n",
    "df_test_original.drop(['flavanoids'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#x and y test for the testing of the model\n",
    "X_test = df_test_original.drop('target', axis=1)\n",
    "y_test = df_test_original['target']\n",
    "\n",
    "# predictiong on the test samples with the trained model\n",
    "y_pred = final_knn.predict(X_test)\n",
    "\n",
    "#results:\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nF1-macro Score: {round(f1_score(y_test, y_pred, average='macro'), 4)}\")\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred), 4)}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nFirst 5 predictions vs true labels:\")\n",
    "for pred, true in zip(y_pred[:5], y_test[:5]):\n",
    "    print(f\"Predicted: {pred}, True: {true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional baseline models and comparison\n",
    "\n",
    "To better understand how our KNN model performs, we compare it with two common baselines:\n",
    "\n",
    "- **Logistic Regression** with feature scaling  \n",
    "- **Random Forest** (tree-based ensemble)\n",
    "\n",
    "We train these models on the same training data and evaluate them on the same test set, then summarize the results in a comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Reuse X_train, y_train, X_test, y_test from previous cells\n",
    "\n",
    "# Logistic Regression pipeline with scaling\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, multi_class='ovr', random_state=42))\n",
    "])\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "\n",
    "# Random Forest (no scaling needed)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Collect metrics\n",
    "def compute_metrics(y_true, y_pred, model_name):\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-macro\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"Precision-macro\": precision_score(y_true, y_pred, average='macro'),\n",
    "        \"Recall-macro\": recall_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(compute_metrics(y_test, y_pred, \"KNN (final model)\"))\n",
    "results.append(compute_metrics(y_test, y_pred_logreg, \"Logistic Regression\"))\n",
    "results.append(compute_metrics(y_test, y_pred_rf, \"Random Forest\"))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Test set performance comparison:\")\n",
    "display(results_df.sort_values(by=\"F1-macro\", ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and future work\n",
    "\n",
    "In this project we implemented a complete supervised learning workflow for wine classification:\n",
    "\n",
    "- Performed EDA to understand feature distributions, correlations, and class balance  \n",
    "- Reduced redundancy by removing a highly correlated feature  \n",
    "- Tuned KNN and Decision Tree models with cross-validation  \n",
    "- Trained a final KNN model and evaluated it on a held-out test set  \n",
    "- Compared the final model with Logistic Regression and Random Forest baselines\n",
    "\n",
    "Possible next steps:\n",
    "\n",
    "- Try more advanced models such as Gradient Boosting or XGBoost  \n",
    "- Perform more systematic hyperparameter tuning (wider search space)  \n",
    "- Apply dimensionality reduction (e.g. PCA) and compare results  \n",
    "- Explore model-agnostic explainability tools (e.g. SHAP) to better understand feature impact\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
